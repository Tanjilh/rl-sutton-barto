Exercise 3.12 The value of a state depends on the the values of the actions
possible in that state and on how likely each action is to be taken under the
current policy. We can think of this in terms of a small backup diagram rooted
at the state and considering each possible action:

Give the equation corresponding to this intuition and diagram for the value at
the root node, vπ(s), in terms of the value at the expected leaf node, qπ(s, a),
given St = s. This expectation depends on the policy, π. Then give a second
equation in which the expected value is written out explicitly in terms of π(a|s)
such that no expected value notation appears in the equation.


vπ(s) = Exp(G_t | S_t = s)
      = Sum_a(π(a|s)*(qπ(s, a)))  

optional (done previously, based on feedback)

-full recursive formula in terms of rewards and next states

qπ(s, a) = Sum_s'(sum_r(p(s′,r∣s,a)⋅[r+γvπ(s′)]))

then implement this into vπ(s)