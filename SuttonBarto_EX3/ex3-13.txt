Exercise 3.13 The value of an action, qπ(s, a), depends on the expected
next reward and the expected sum of the remaining rewards. Again we can
think of this in terms of a small backup diagram, this one rooted at an action
(state–action pair) and branching to the possible next states:

Give the equation corresponding to this intuition and diagram for the action
value, qπ(s, a), in terms of the expected next reward, Rt+1, and the expected
next state value, vπ(St+1), given that St = s and At = a. Then give a second
equation, writing out the expected value explicitly in terms of p(s', r|s, a)
defined by (3.6), such that no expected value notation appears in the equation.


qπ(s, a) = Exp(G_t | S_t = s, A_t = a)
         = Exp(R_t+1 | S_t = s, A_t = a) + Exp(y * G_t+1 | S_t = s, A_t = a)
         = sum_a(π(a|s) * sum_s'(sum_r(p(s',r | s, a) * [r]))) 
           + sum_a(π(a|s) * sum_s'(sum_r(p(s',r | s, a) * [y * Exp(sum_k(y^k *R_t+k+2)| S_t+1 = s')])))
         = sum_a(π(a|s) * sum_s'(sum_r(p(s',r | s, a) * [r + y * Exp(sum_k(y^k *R_t+k+2)| S_t+1 = s')])))
         = sum_a(π(a|s) * sum_s'(sum_r(p(s',r | s, a) * [r + y * vπ(s')])))

answer (based on feedback)

qπ(s, a) = Exp(G_t | S_t = s, A_t = a)
         = Exp(R_t+1 | S_t = s, A_t = a) + Exp(y * G_t+1 | S_t = s, A_t = a)
         = sum_s'(sum_r(p(s',r | s, a) * [r]))
           + sum_s'(sum_r(p(s',r | s, a) * [y * Exp(sum_k(y^k *R_t+k+2)| S_t+1 = s')]))
         = sum_s'(sum_r(p(s',r | s, a) * [r + y * Exp(sum_k(y^k *R_t+k+2)| S_t+1 = s')]))
         = sum_s'(sum_r(p(s',r | s, a) * [r + y * vπ(s')]))

we dont need to use the policy of a and sum over all possible actions
in the state since we are already conditioning on action a





