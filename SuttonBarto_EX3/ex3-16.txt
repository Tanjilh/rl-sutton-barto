Exercise 3.16 Give the Bellman equation for qâˆ— for the recycling robot.

optimal policy would entail : searching when battery states are high, then waiting 
for any extra recycleables when low, before recharging when low to then repeat...

assumimg we start in a high state and actions are deterministic ==> p(s',r | s, a) = 1

q*Ï€(s,a) = sum_s'(sum_r(p(s',r | s, a)*[R_t+1 + y * max_a'(q*(s', a'))]))

where s = h ,  a = search ==> s' = l , a = wait , ==> s'' = l , a = recharge

then repeating the cycle in a continuing model.

answer (based on feedback)

weâ€™re not evaluating a specific path like H â†’ L â†’ recharge, but rather computing the expected 
maximum return over all possible next actions from ğ‘ â€².

h stays in h with probability x and goes to L with probability 1-x

so
q*Ï€(s,a) = sum_s'(sum_r(p(s',r | s, a)*[R_t+1 + y * max_a'(q*(s', a'))]))

q*Ï€(h,search) = x * [R_search + y*max_a'(q*(H, a'))] + (1-x) * [R_search + y *max_a'(q*(L, a'))]

waiting leaves the state deterministic.

q*Ï€(h,wait) = R_wait + y*max_a'(q*(H, a')) 

lets say the chance of failure in a L state is 1-k and succeeds with probability k

q*Ï€(L,search) = k * [R_success + y*max_a'(q*(L, a'))] + (1-k) * [R_fail + y*max_a'(q*(L, a'))]

q*Ï€(L,wait) = R_wait + y*max_a'(q*(L, a'))

q*Ï€(L,recharge) = R_recharge + y*max_a'(q*(H, a'))

